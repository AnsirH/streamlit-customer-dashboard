import pandas as pd
import numpy as np
from pathlib import Path
from utils.cache import load_model
from utils.logger import setup_logger
from config import PATHS, MODEL_CONFIG
import joblib
import pickle
import os
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go

logger = setup_logger(__name__)

########## í•¨ìˆ˜ì—…ë°ì´íŠ¸ì‘ì—… ##########

class ChurnPredictor:
    """ê³ ê° ì´íƒˆ ì˜ˆì¸¡ì„ ìœ„í•œ ëª¨ë¸ í´ë˜ìŠ¤"""
    
    def __init__(self, model_path=None):
        """ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.model = None
        if model_path is None:
            self.model_path = Path(__file__).parent / "xgboost_best_model.pkl"
        else:
            self.model_path = model_path
        self.feature_importance_cache = None  # íŠ¹ì„± ì¤‘ìš”ë„ ìºì‹œ ì¶”ê°€
        try:
            self.load_model()
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
            st.error(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    
    def load_model(self):
        """ëª¨ë¸ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            if not self.model_path.exists():
                logger.error(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
                return False
            
            self.model = joblib.load(self.model_path)
            logger.info(f"ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {self.model_path}")
            # ë””ë²„ê·¸ ì¶œë ¥ ì¶”ê°€
            st.write(f"ğŸ” ë””ë²„ê·¸: ëª¨ë¸ ë¡œë“œ ì„±ê³µ - {self.model_path}")
            return True
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            # ë””ë²„ê·¸ ì¶œë ¥ ì¶”ê°€
            st.error(f"ğŸ” ë””ë²„ê·¸: ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - {str(e)}")
            return False
    
    def predict(self, input_df):
        """
        ì´íƒˆ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            input_df (pandas.DataFrame): ì˜ˆì¸¡í•  ê³ ê° ë°ì´í„°
            
        Returns:
            tuple: (ì˜ˆì¸¡ í´ë˜ìŠ¤, ì´íƒˆ í™•ë¥ ) ë˜ëŠ” ì˜¤ë¥˜ ë°œìƒ ì‹œ (None, None)
        """
        try:
            # 1. ì…ë ¥ ë°ì´í„° ê²€ì¦
            if input_df is None or len(input_df) == 0:
                st.error("âš ï¸ ì˜ˆì¸¡í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None, None
                
            # 2. ëª¨ë¸ ê²€ì¦
            if self.model is None:
                st.error("âš ï¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                if self.load_model() is False:
                    return None, None
            
            # 3. ì…ë ¥ ë°ì´í„°ì˜ ë²”ì£¼í˜• ë³€ìˆ˜ ê°’ì´ ëª¨ë¸ê³¼ í˜¸í™˜ë˜ëŠ”ì§€ ë¯¸ë¦¬ ê²€ì¦
            safe_prediction, unknown_values = self._validate_categorical_values(input_df)
            
            # 4. ëª¨ë¸ì´ ì¸ì‹í•˜ì§€ ëª»í•˜ëŠ” ë²”ì£¼ê°’ì´ ìˆëŠ” ê²½ìš°
            if not safe_prediction:
                # 4.1 ì—„ê²©í•œ ëª¨ë“œ: ì˜ˆì¸¡ ì¤‘ë‹¨
                if self.strict_mode:
                    st.error("âš ï¸ ëª¨ë¸ì´ ì¸ì‹í•˜ì§€ ëª»í•˜ëŠ” ë²”ì£¼ê°’ì´ ìˆì–´ ì˜ˆì¸¡ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    for cat, values in unknown_values.items():
                        st.error(f"- '{cat}' ë³€ìˆ˜ì— ì•Œ ìˆ˜ ì—†ëŠ” ê°’: {values}")
                    return None, None
                # 4.2 ìœ ì—°í•œ ëª¨ë“œ: ê²½ê³  í‘œì‹œ í›„ ê³„ì† ì§„í–‰ (ìë™ ëŒ€ì²´)
                else:
                    st.warning("âš ï¸ ëª¨ë¸ì´ ì¸ì‹í•˜ì§€ ëª»í•˜ëŠ” ë²”ì£¼ê°’ì´ ìˆìŠµë‹ˆë‹¤. ìë™ ëŒ€ì²´ ì‚¬ìš©.")
                    for cat, values in unknown_values.items():
                        st.warning(f"- '{cat}' ë³€ìˆ˜ì— ì•Œ ìˆ˜ ì—†ëŠ” ê°’: {values}")
            
            # 5. ë°ì´í„° ì „ì²˜ë¦¬
            processed_df = self._preprocess_data(input_df)
            
            # 6. ì˜ˆì¸¡ ìˆ˜í–‰
            try:
                y_pred = self.model.predict(processed_df)
                y_proba = self.model.predict_proba(processed_df)[:, 1]  # ì´íƒˆ í™•ë¥ 
                
                # 7. ë¶ˆí™•ì‹¤ì„± ì •ë³´ ì¶”ê°€ (ëª¨ë¸ì´ ì¸ì‹í•˜ì§€ ëª»í•˜ëŠ” ë²”ì£¼ê°’ì´ ìˆëŠ” ê²½ìš°)
                if not safe_prediction and not self.strict_mode:
                    # 7.1 ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„± ê²½ê³ 
                    st.warning("âš ï¸ ëª¨ë¸ì´ ì¸ì‹í•˜ì§€ ëª»í•˜ëŠ” ë²”ì£¼ê°’ì´ ìˆì–´ ì˜ˆì¸¡ ê²°ê³¼ì˜ ì‹ ë¢°ë„ê°€ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    # 7.2 ë¶ˆí™•ì‹¤ì„±ì„ ë°˜ì˜í•œ í™•ë¥  ì¡°ì • (ì„ íƒ ì‚¬í•­)
                    # y_proba = self._adjust_probability_for_uncertainty(y_proba, unknown_values)
                
                # 8. ì˜ˆì¸¡ ì„±ê³µ ì‹œ íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° (ì„ íƒ ì‚¬í•­)
                try:
                    self._compute_feature_importance(processed_df)
                except Exception as e:
                    # íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° ì‹¤íŒ¨í•´ë„ ì˜ˆì¸¡ ê²°ê³¼ëŠ” ë°˜í™˜
                    pass
                
                return y_pred, y_proba
                
            except Exception as e:
                st.error(f"âš ï¸ ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                return None, None
        
        except Exception as e:
            st.error(f"âš ï¸ ì˜ˆì¸¡ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            st.write(f"ğŸ” [ì˜ˆì¸¡ ì˜¤ë¥˜]: {traceback.format_exc()}")
            return None, None
    
    def _validate_categorical_values(self, input_df):
        """
        ì…ë ¥ ë°ì´í„°ì˜ ë²”ì£¼í˜• ë³€ìˆ˜ ê°’ì´ ëª¨ë¸ê³¼ í˜¸í™˜ë˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
        
        Args:
            input_df (pd.DataFrame): ê²€ì¦í•  ì…ë ¥ ë°ì´í„°
            
        Returns:
            tuple: (ì•ˆì „í•œ ì˜ˆì¸¡ ì—¬ë¶€, ì•Œ ìˆ˜ ì—†ëŠ” ê°’ ëª©ë¡)
        """
        # 1. ê¸°ë³¸ê°’ ì„¤ì •
        is_safe = True
        unknown_values = {}
        
        # 2. ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° ê²€ì¦ ë¶ˆê°€
        if self.model is None or not hasattr(self.model, 'feature_names_in_'):
            return True, {}  # ê²€ì¦ ë¶ˆê°€í•˜ë¯€ë¡œ ì•ˆì „í•˜ë‹¤ê³  ê°€ì •
            
        # 3. ì›í•«ì¸ì½”ë”©ëœ íŠ¹ì„± êµ¬ì¡° ê°€ì ¸ì˜¤ê¸°
        encoded_features = self.get_onehot_encoded_features()
        
        # 4. ê° ë²”ì£¼í˜• ë³€ìˆ˜ë³„ ê°€ëŠ¥í•œ ê°’ ëª©ë¡ ì¶”ì¶œ
        category_values = {}
        for prefix, features in encoded_features.items():
            category_values[prefix] = [f.split('_', 1)[1] for f in features]
        
        # 5. ì…ë ¥ ë°ì´í„°ì˜ ê° ë²”ì£¼í˜• ë³€ìˆ˜ ê²€ì¦
        for prefix in encoded_features.keys():
            if prefix in input_df.columns:
                # 5.1 ì…ë ¥ ë°ì´í„°ì˜ í•´ë‹¹ ì»¬ëŸ¼ ê³ ìœ ê°’ ê°€ì ¸ì˜¤ê¸°
                input_values = input_df[prefix].astype(str).str.strip().unique()
                
                # 5.2 ëª¨ë¸ì´ ì¸ì‹í•˜ì§€ ëª»í•˜ëŠ” ê°’ ì°¾ê¸°
                possible_values = [str(v).lower() for v in category_values.get(prefix, [])]
                unknown = [val for val in input_values 
                          if str(val).lower() not in possible_values]
                
                # 5.3 ì•Œ ìˆ˜ ì—†ëŠ” ê°’ì´ ìˆìœ¼ë©´ ê¸°ë¡
                if unknown:
                    is_safe = False
                    unknown_values[prefix] = unknown
        
        return is_safe, unknown_values

    def _preprocess_data(self, data):
        """
        ëª¨ë¸ ì˜ˆì¸¡ì„ ìœ„í•´ ì…ë ¥ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
        ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” ì •í™•í•œ íŠ¹ì„± êµ¬ì¡°ì™€ ì¼ì¹˜í•˜ëŠ” ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            data (pd.DataFrame): ì „ì²˜ë¦¬í•  ì…ë ¥ ë°ì´í„°
            
        Returns:
            pd.DataFrame: ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ ì „ì²˜ë¦¬ëœ ë°ì´í„°
        """
        try:
            # 0. ëª¨ë¸ ì„¤ì • í™•ì¸
            self.strict_mode = False  # ê¸°ë³¸ì ìœ¼ë¡œ ìœ ì—°í•œ ëª¨ë“œ ì‚¬ìš© (ì˜ˆì¸¡ ê³„ì† ì§„í–‰)
            
            # 1. ì›ë³¸ ë°ì´í„° ë¡œê¹…
            st.write(f"ğŸ” [ì „ì²˜ë¦¬]: ì›ë³¸ ë°ì´í„° í¬ê¸°: {data.shape}")
            
            # 2. ëª¨ë¸ í™•ì¸
            if self.model is None:
                st.error("âš ï¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return data.copy()
                
            if not hasattr(self.model, 'feature_names_in_'):
                st.error("âš ï¸ ëª¨ë¸ì— 'feature_names_in_' ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤.")
                return data.copy()
            
            # 3. ëª¨ë¸ íŠ¹ì„± ê°€ì ¸ì˜¤ê¸°
            model_features = list(self.model.feature_names_in_)
            st.write(f"ğŸ” [ì „ì²˜ë¦¬]: ëª¨ë¸ íŠ¹ì„± ìˆ˜: {len(model_features)}")
            
            # 4. ì›í•«ì¸ì½”ë”© êµ¬ì¡° ë¶„ì„
            encoded_features_dict = self.get_onehot_encoded_features()
            
            # 5. ë²”ì£¼í˜• ë³€ìˆ˜ ì‹ë³„ (ì›í•«ì¸ì½”ë”©ëœ íŠ¹ì„±ì—ì„œ ì¶”ì¶œ)
            categorical_cols = list(encoded_features_dict.keys())
            st.write(f"ğŸ” [ì „ì²˜ë¦¬]: ì›í•«ì¸ì½”ë”© ëŒ€ìƒ ë²”ì£¼í˜• ë³€ìˆ˜: {categorical_cols}")
            
            # 6. ê° ë²”ì£¼í˜• ë³€ìˆ˜ë³„ ê°€ëŠ¥í•œ ê°’ ëª©ë¡ ì¶”ì¶œ
            category_values = {}
            for prefix, features in encoded_features_dict.items():
                category_values[prefix] = [f.split('_', 1)[1] for f in features]
            
            # 7. ìƒˆ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„± (ëª¨ë¸ì´ í•„ìš”ë¡œ í•˜ëŠ” ëª¨ë“  íŠ¹ì„±ì„ í¬í•¨í•˜ê²Œ ë¨)
            result_df = pd.DataFrame(index=data.index)
            
            # 8. ë¯¸ë¦¬ ëª¨ë“  ì›í•«ì¸ì½”ë”© ì»¬ëŸ¼ì„ 0ìœ¼ë¡œ ì´ˆê¸°í™”
            for feature in model_features:
                result_df[feature] = 0
            
            # 9. ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬ ë° ì„ì˜ ê°’ ë§¤í•‘ ê¸°ë¡
            fallback_mappings = {}  # ë§¤í•‘ ì •ë³´ ì €ì¥
            
            # 10. ê° ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬
            for prefix in categorical_cols:
                if prefix in data.columns:
                    # 10.1 ì…ë ¥ ë°ì´í„°ì—ì„œ í•´ë‹¹ ì»¬ëŸ¼ ê°’ ê°€ì ¸ì˜¤ê¸°
                    for idx, row in data.iterrows():
                        input_value = str(row[prefix]).strip()
                        
                        # 10.2 í•´ë‹¹ ê°’ì— ëŒ€í•œ ì›í•«ì¸ì½”ë”© ì»¬ëŸ¼ ì´ë¦„ ìƒì„±
                        expected_col = f"{prefix}_{input_value}"
                        
                        # 10.3 í•´ë‹¹ ì»¬ëŸ¼ì´ ëª¨ë¸ íŠ¹ì„±ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                        col_exists = expected_col in model_features
                        
                        if col_exists:
                            # 10.4 ì¡´ì¬í•˜ëŠ” ê²½ìš° í•´ë‹¹ ì»¬ëŸ¼ì„ 1ë¡œ ì„¤ì •
                            result_df.loc[idx, expected_col] = 1
                        else:
                            # 10.5 ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš° ëŒ€ì²´ ì „ëµ ì ìš©
                            st.warning(f"âš ï¸ '{prefix}'ì˜ ê°’ '{input_value}'ì— ëŒ€í•œ ì›í•«ì¸ì½”ë”© ì»¬ëŸ¼ì´ ëª¨ë¸ì— ì—†ìŠµë‹ˆë‹¤.")
                            
                            if prefix in category_values and category_values[prefix]:
                                # 10.6 ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤í•‘ ì‹œë„
                                possible_values = category_values[prefix]
                                
                                # 10.6.1 ë°©ë²• 1: ì •í™•íˆ ëŒ€ì†Œë¬¸ìë§Œ ë‹¤ë¥¸ ê²½ìš°
                                case_insensitive_match = next((val for val in possible_values 
                                                       if val.lower() == input_value.lower()), None)
                                
                                if case_insensitive_match:
                                    # ëŒ€ì†Œë¬¸ì ì°¨ì´ë§Œ ìˆëŠ” ë§¤ì¹­ ì‚¬ìš©
                                    fallback_col = f"{prefix}_{case_insensitive_match}"
                                    result_df.loc[idx, fallback_col] = 1
                                    fallback_mappings[input_value] = case_insensitive_match
                                    st.write(f"âœ“ ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ë§¤í•‘: '{input_value}' â†’ '{case_insensitive_match}'")
                                else:
                                    # 10.6.2 ë°©ë²• 2: ì²« ë²ˆì§¸ ê°’ ì‚¬ìš© (ê¸°ë³¸ê°’)
                                    fallback_value = possible_values[0]  # ì²« ë²ˆì§¸ ê°’ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
                                    fallback_col = f"{prefix}_{fallback_value}"
                                    result_df.loc[idx, fallback_col] = 1
                                    fallback_mappings[input_value] = fallback_value
                                    st.write(f"âœ“ ê¸°ë³¸ê°’ ë§¤í•‘: '{input_value}' â†’ '{fallback_value}' (ì²« ë²ˆì§¸ ê°’)")
                            else:
                                st.error(f"âš ï¸ '{prefix}'ì— ëŒ€í•œ ê°€ëŠ¥í•œ ê°’ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            
            # 11. ë²”ì£¼í˜•ì´ ì•„ë‹Œ ì¼ë°˜ íŠ¹ì„± ì²˜ë¦¬
            for feature in model_features:
                # ì›í•«ì¸ì½”ë”©ëœ íŠ¹ì„±ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì²˜ë¦¬ (ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ê²½ìš°)
                if '_' not in feature and feature in data.columns:
                    result_df[feature] = data[feature]
            
            # 12. ë””ë²„ê¹…: ëŒ€ì²´ ë§¤í•‘ ì •ë³´ ì¶œë ¥
            if fallback_mappings:
                st.write("### âš ï¸ ì›í•«ì¸ì½”ë”© ê°’ ëŒ€ì²´ ì •ë³´")
                for original, replacement in fallback_mappings.items():
                    st.write(f"- '{original}' â†’ '{replacement}'")
            
            # 13. ê²°ê³¼ ë°ì´í„° ê²€ì¦
            if set(result_df.columns) != set(model_features):
                st.error("âš ï¸ ìƒì„±ëœ íŠ¹ì„±ì´ ëª¨ë¸ íŠ¹ì„±ê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
                missing = set(model_features) - set(result_df.columns)
                extra = set(result_df.columns) - set(model_features)
                
                if missing:
                    st.write(f"- ëˆ„ë½ëœ íŠ¹ì„±: {list(missing)}")
                if extra:
                    st.write(f"- ì¶”ê°€ëœ íŠ¹ì„±: {list(extra)}")
                    result_df = result_df.drop(columns=list(extra))
                
                # ëˆ„ë½ëœ íŠ¹ì„± ì¶”ê°€
                for feat in missing:
                    result_df[feat] = 0
            
            # 14. ëª¨ë¸ íŠ¹ì„± ìˆœì„œì— ë§ê²Œ ì¬ì •ë ¬
            result_df = result_df[model_features]
            
            # 15. ê° ë²”ì£¼í˜• ë³€ìˆ˜ë³„ í™œì„±í™”ëœ ì»¬ëŸ¼ í™•ì¸
            if len(data) == 1:  # ë‹¨ì¼ ë ˆì½”ë“œì¸ ê²½ìš°
                for prefix in categorical_cols:
                    if prefix in data.columns:
                        # í˜„ì¬ ê°’ í™•ì¸
                        input_value = data[prefix].iloc[0]
                        
                        # ì´ ë²”ì£¼í˜• ë³€ìˆ˜ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ì›í•«ì¸ì½”ë”© ì»¬ëŸ¼
                        ohe_cols = [col for col in result_df.columns if col.startswith(f"{prefix}_")]
                        
                        # í™œì„±í™”ëœ ì»¬ëŸ¼ (ê°’ì´ 1ì¸ ì»¬ëŸ¼)
                        active_cols = [col for col in ohe_cols if result_df[col].iloc[0] == 1]
                        
                        if active_cols:
                            # í™œì„±í™”ëœ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°
                            active_values = [col.split('_', 1)[1] for col in active_cols]
                            st.write(f"âœ“ '{prefix}': ì…ë ¥ê°’ '{input_value}' â†’ í™œì„±í™”ëœ ê°’: {active_values}")
                        else:
                            # í™œì„±í™”ëœ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° (ì¤‘ìš”í•œ ì˜¤ë¥˜!)
                            st.error(f"âš ï¸ '{prefix}'ì— ëŒ€í•´ í™œì„±í™”ëœ ì›í•«ì¸ì½”ë”© ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
                            st.write(f"  - ì›ë³¸ ê°’: '{input_value}'")
                            st.write(f"  - ê°€ëŠ¥í•œ ê°’: {category_values.get(prefix, [])}")
            
            # 16. ì „ì²˜ë¦¬ ì™„ë£Œ
            st.write(f"ğŸ” [ì „ì²˜ë¦¬]: ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ, ìµœì¢… í¬ê¸°: {result_df.shape}")
            
            return result_df
            
        except Exception as e:
            st.error(f"âš ï¸ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            st.write(f"ğŸ” [ì „ì²˜ë¦¬ ì˜¤ë¥˜]: {traceback.format_exc()}")
            return data.copy()
    
    def get_onehot_encoded_features(self):
        """
        ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì›í•«ì¸ì½”ë”©ëœ íŠ¹ì„± ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Returns:
            dict: ë²”ì£¼í˜• ë³€ìˆ˜ë³„ ì›í•«ì¸ì½”ë”©ëœ íŠ¹ì„± ëª©ë¡ (ì˜ˆ: {'Gender': ['Gender_Male', 'Gender_Female']})
        """
        # 1. ê¸°ë³¸ ê²€ì¦
        if self.model is None:
            st.warning("âš ï¸ ëª¨ë¸ì´ ì—†ì–´ ì›í•«ì¸ì½”ë”© íŠ¹ì„±ì„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}
            
        if not hasattr(self.model, 'feature_names_in_'):
            st.warning("âš ï¸ ëª¨ë¸ì— feature_names_in_ ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        # 2. ëª¨ë¸ì˜ ëª¨ë“  íŠ¹ì„± ê°€ì ¸ì˜¤ê¸°
        all_features = list(self.model.feature_names_in_)
        st.write(f"ğŸ” ë””ë²„ê·¸ [ì›í•«ì¸ì½”ë”© ë¶„ì„]: ëª¨ë¸ ì „ì²´ íŠ¹ì„± ìˆ˜: {len(all_features)}")
        
        # 3. ëª¨ë“  íŠ¹ì„±ì—ì„œ ì ‘ë‘ì‚¬-ê°’ íŒ¨í„´ ì°¾ê¸°
        pattern_features = []
        for feature in all_features:
            if '_' in feature:
                pattern_features.append(feature)
        
        st.write(f"ğŸ” ë””ë²„ê·¸ [ì›í•«ì¸ì½”ë”© ë¶„ì„]: '_'ê°€ í¬í•¨ëœ íŠ¹ì„± ìˆ˜: {len(pattern_features)}")
        
        if not pattern_features:
            st.warning("âš ï¸ ì›í•«ì¸ì½”ë”© íŒ¨í„´ì„ ê°€ì§„ íŠ¹ì„±ì´ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ì´ ì›í•«ì¸ì½”ë”©ì„ ì‚¬ìš©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            # ëª¨ë¸ íŠ¹ì„± ìƒ˜í”Œ ì¶œë ¥
            st.write(f"ğŸ” ë””ë²„ê·¸ [ì›í•«ì¸ì½”ë”© ë¶„ì„]: ëª¨ë¸ íŠ¹ì„± ìƒ˜í”Œ: {all_features[:10]}")
            return {}
        
        # 4. ëª¨ë“  ì ‘ë‘ì‚¬ ì¶”ì¶œ ë° ê°œìˆ˜ ì¹´ìš´íŠ¸
        prefix_counter = {}
        for feature in pattern_features:
            prefix = feature.split('_')[0]
            if prefix not in prefix_counter:
                prefix_counter[prefix] = []
            prefix_counter[prefix].append(feature)
        
        # 5. ì ‘ë‘ì‚¬ë³„ í†µê³„ ì¶œë ¥ (ë””ë²„ê¹…)
        st.write(f"ğŸ” ë””ë²„ê·¸ [ì›í•«ì¸ì½”ë”© ë¶„ì„]: ë°œê²¬ëœ ì ‘ë‘ì‚¬: {list(prefix_counter.keys())}")
        
        # 6. ì›í•«ì¸ì½”ë”©ìœ¼ë¡œ ê°„ì£¼í•  ìµœì†Œ ë³€ìˆ˜ ìˆ˜ (ì ì–´ë„ 2ê°œ ì´ìƒì´ì–´ì•¼ í•¨)
        min_variants = 2
        
        # 7. ì›í•«ì¸ì½”ë”© ë³€ìˆ˜ ê·¸ë£¹í™”
        encoded_features = {}
        for prefix, features in prefix_counter.items():
            if len(features) >= min_variants:
                # ì ‘ë‘ì‚¬ê°€ ì¼ë°˜ì ì¸ ë²”ì£¼í˜• ë³€ìˆ˜ëª…ì¸ì§€ í™•ì¸
                is_valid_prefix = any(common in prefix.lower() for common in [
                    'gender', 'sex', 'marital', 'city', 'status', 'login', 'device', 
                    'payment', 'order', 'cat', 'category', 'type', 'satisfaction', 
                    'education', 'income', 'occupation', 'complain'
                ])
                
                if is_valid_prefix or len(features) >= 3:  # ì˜ ì•Œë ¤ì§„ ë²”ì£¼í˜• ë³€ìˆ˜ëª…ì´ê±°ë‚˜ 3ê°œ ì´ìƒì˜ ë³€í˜•ì´ ìˆëŠ” ê²½ìš°
                    encoded_features[prefix] = features
                    category_values = [f.split('_', 1)[1] for f in features]
                    
                    st.write(f"âœ… ë””ë²„ê·¸ [ì›í•«ì¸ì½”ë”© ë¶„ì„]: ë²”ì£¼í˜• ë³€ìˆ˜ '{prefix}' í™•ì¸ë¨")
                    st.write(f"  - ì»¬ëŸ¼ ìˆ˜: {len(features)}")
                    st.write(f"  - ë²”ì£¼ê°’: {category_values}")
        
        # 8. ì›í•«ì¸ì½”ë”© ë³€ìˆ˜ê°€ ë°œê²¬ë˜ì§€ ì•Šì€ ê²½ìš° 
        if not encoded_features:
            st.warning("âš ï¸ ì›í•«ì¸ì½”ë”© ë³€ìˆ˜ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            
            # ë°œê²¬ëœ ëª¨ë“  ì ‘ë‘ì‚¬ ì •ë³´ ì¶œë ¥
            for prefix, features in prefix_counter.items():
                st.write(f"ğŸ” ë””ë²„ê·¸ [ì›í•«ì¸ì½”ë”© ë¶„ì„]: ì ‘ë‘ì‚¬ '{prefix}': {len(features)}ê°œ ì»¬ëŸ¼")
                if len(features) <= 5:  # ì»¬ëŸ¼ ìˆ˜ê°€ ì ì€ ê²½ìš° ëª¨ë‘ í‘œì‹œ
                    st.write(f"  - ì»¬ëŸ¼: {features}")
                else:
                    st.write(f"  - ì»¬ëŸ¼ ìƒ˜í”Œ: {features[:5]}...")
            
            # íŠ¹ì„±ëª… êµ¬ì¡° ë¶„ì„
            uppercase_count = sum(1 for f in all_features if any(c.isupper() for c in f))
            digit_count = sum(1 for f in all_features if any(c.isdigit() for c in f))
            
            st.write(f"ğŸ” ë””ë²„ê·¸ [ì›í•«ì¸ì½”ë”© ë¶„ì„]: ëŒ€ë¬¸ì í¬í•¨ íŠ¹ì„± ìˆ˜: {uppercase_count}")
            st.write(f"ğŸ” ë””ë²„ê·¸ [ì›í•«ì¸ì½”ë”© ë¶„ì„]: ìˆ«ì í¬í•¨ íŠ¹ì„± ìˆ˜: {digit_count}")
            
            # ëª¨ë¸ ë‚´ì¥ ì¤‘ìš”ë„ ì •ë³´ê°€ ìˆë‹¤ë©´ ìƒìœ„ íŠ¹ì„± ì¶œë ¥
            if hasattr(self.model, 'feature_importances_'):
                importances = self.model.feature_importances_
                indices = np.argsort(importances)[-5:]  # ìƒìœ„ 5ê°œ
                st.write("ğŸ” ë””ë²„ê·¸ [ì›í•«ì¸ì½”ë”© ë¶„ì„]: ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„± Top 5:")
                for i in reversed(indices):
                    st.write(f"  - {all_features[i]}: {importances[i]:.4f}")
            
            # ìˆ˜ë™ìœ¼ë¡œ ëª‡ ê°€ì§€ ë²”ì£¼í˜• ë³€ìˆ˜ ì¶”ê°€
            st.write("ğŸ” ë””ë²„ê·¸ [ì›í•«ì¸ì½”ë”© ë¶„ì„]: ì¼ë°˜ì ì¸ ë²”ì£¼í˜• ë³€ìˆ˜ëª… ê²€ìƒ‰ ì¤‘...")
            common_categories = ['Gender', 'MaritalStatus', 'LoginDevice', 'PaymentMode', 'OrderCat']
            
            for category in common_categories:
                matching = [f for f in all_features if category.lower() in f.lower()]
                if matching:
                    st.write(f"ğŸ” ë””ë²„ê·¸ [ì›í•«ì¸ì½”ë”© ë¶„ì„]: '{category}' ê´€ë ¨ íŠ¹ì„± ë°œê²¬: {len(matching)}ê°œ")
                    st.write(f"  - íŠ¹ì„±: {matching}")
                    # ê´€ë ¨ íŠ¹ì„±ì´ ë°œê²¬ë˜ë©´ encoded_featuresì— ì¶”ê°€
                    encoded_features[category] = matching
            
            # ê·¸ë˜ë„ ë°œê²¬ë˜ì§€ ì•Šìœ¼ë©´ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ êµ¬ë¶„ëœ ëª¨ë“  íŠ¹ì„± ì¶”ê°€
            if not encoded_features and pattern_features:
                st.write("ğŸ” ë””ë²„ê·¸ [ì›í•«ì¸ì½”ë”© ë¶„ì„]: ìµœí›„ì˜ ë°©ë²•ìœ¼ë¡œ ëª¨ë“  '_' í¬í•¨ íŠ¹ì„±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")
                for prefix, features in prefix_counter.items():
                    if len(features) >= min_variants:
                        encoded_features[prefix] = features
        
        return encoded_features
    
    def _compute_feature_importance(self, input_data):
        """Calculate feature importance for a prediction."""
        try:
            # ìºì‹œëœ íŠ¹ì„± ì¤‘ìš”ë„ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            if self.feature_importance_cache is not None:
                return self.feature_importance_cache
                
            # ì´í•˜ ê¸°ì¡´ ë¡œì§
            if self.model is None:
                self.load_model()
                
            if self.model is None:  # ì—¬ì „íˆ Noneì´ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
                return self._default_feature_importance()
                
            # SHAP ì‚¬ìš© ì‹œë„
            try:
                import shap
                explainer = shap.TreeExplainer(self.model)
                shap_values = explainer.shap_values(input_data)
                
                # ë¶„ë¥˜ ëª¨ë¸ì¸ ê²½ìš° í´ë˜ìŠ¤ 1(ì´íƒˆ)ì— ëŒ€í•œ SHAP ê°’ ì„ íƒ
                if isinstance(shap_values, list):
                    shap_values = shap_values[1]
                    
                # ì ˆëŒ€ê°’ ì·¨í•´ ì¤‘ìš”ë„ ê³„ì‚°
                importance = np.abs(shap_values).mean(axis=0)
                
                # ìºì‹œì— ì €ì¥
                self.feature_importance_cache = importance
                return importance
                
            except Exception as e:
                print(f"SHAPì„ ì‚¬ìš©í•œ íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
                
                # ëª¨ë¸ì— feature_importances_ ì†ì„±ì´ ìˆìœ¼ë©´ ì‚¬ìš©
                if hasattr(self.model, 'feature_importances_'):
                    importance = self.model.feature_importances_
                    self.feature_importance_cache = importance
                    return importance
                    
                return self._default_feature_importance()
                
        except Exception as e:
            print(f"íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return self._default_feature_importance()
    
    def _default_feature_importance(self):
        """íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°ì´ ì‹¤íŒ¨í•  ê²½ìš° ê¸°ë³¸ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        # ê¸°ë³¸ íŠ¹ì„± ì¤‘ìš”ë„ ê°’ ì„¤ì •
        default_importance = np.array([0.25, 0.22, 0.18, 0.15, 0.12, 0.08])
        self.feature_importance_cache = default_importance
        return default_importance
    
    def get_feature_importance(self):
        """ìºì‹œëœ íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if self.feature_importance_cache is None:
            # íŠ¹ì„± ì¤‘ìš”ë„ê°€ ê³„ì‚°ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
            return self._default_feature_importance()
            
        # íŠ¹ì„± ì¤‘ìš”ë„ê°€ ë°°ì—´ í˜•íƒœì¸ ê²½ìš° ì‚¬ì „ í˜•íƒœë¡œ ë³€í™˜
        if isinstance(self.feature_importance_cache, np.ndarray):
            # íŠ¹ì„± ì´ë¦„ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì´ë¦„ ì‚¬ìš©
            features = {}
            for i, val in enumerate(self.feature_importance_cache):
                features[f'feature_{i+1}'] = float(val)
            return features
            
        return self.feature_importance_cache

    def get_model_features(self):
        """
        ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” íŠ¹ì„±(ì»¬ëŸ¼) ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            dict: ëª¨ë¸ íŠ¹ì„±ì— ëŒ€í•œ ìƒì„¸ ì •ë³´
        """
        if self.model is None:
            st.error("âš ï¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {"error": "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
            
        if not hasattr(self.model, 'feature_names_in_'):
            st.error("âš ï¸ ëª¨ë¸ì— feature_names_in_ ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {"error": "ëª¨ë¸ì— feature_names_in_ ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤"}
            
        # ëª¨ë¸ì˜ ëª¨ë“  íŠ¹ì„±
        all_features = list(self.model.feature_names_in_)
        
        # ê²°ê³¼ ì •ë³´ êµ¬ì„±
        result = {
            "total_features": len(all_features),
            "features": all_features,
            "sample_features": all_features[:10] if len(all_features) > 10 else all_features
        }
        
        # ì›í•«ì¸ì½”ë”© íŠ¹ì„± ê·¸ë£¹í™”
        encoded_features = self.get_onehot_encoded_features()
        result["onehot_groups"] = len(encoded_features)
        result["onehot_features"] = encoded_features
        
        # ì›í•«ì¸ì½”ë”© ë˜ì§€ ì•Šì€ íŠ¹ì„±
        all_encoded = []
        for group in encoded_features.values():
            all_encoded.extend(group)
        
        non_encoded = [f for f in all_features if f not in all_encoded]
        result["non_encoded_features"] = non_encoded
        
        # íŠ¹ì„± íƒ€ì…ë³„ ë¶„ë¥˜
        has_underscore = [f for f in all_features if '_' in f]
        has_digit = [f for f in all_features if any(c.isdigit() for c in f)]
        has_uppercase = [f for f in all_features if any(c.isupper() for c in f)]
        
        result["has_underscore_count"] = len(has_underscore)
        result["has_digit_count"] = len(has_digit)
        result["has_uppercase_count"] = len(has_uppercase)
        
        # ì¤‘ìš”ë„ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° 
        if hasattr(self.model, 'feature_importances_'):
            importances = self.model.feature_importances_
            top_indices = np.argsort(importances)[-10:]  # ìƒìœ„ 10ê°œ
            top_features = [(all_features[i], float(importances[i])) for i in reversed(top_indices)]
            result["top_features"] = top_features
            
        return result
    
    def print_model_features(self):
        """
        ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” íŠ¹ì„± ì •ë³´ë¥¼ í™”ë©´ì— ì¶œë ¥í•©ë‹ˆë‹¤.
        """
        features_info = self.get_model_features()
        
        if "error" in features_info:
            st.error(features_info["error"])
            return
            
        st.write("## ëª¨ë¸ íŠ¹ì„± ì •ë³´")
        st.write(f"- ì´ íŠ¹ì„± ìˆ˜: {features_info['total_features']}")
        
        # íŠ¹ì„± ìƒ˜í”Œ í‘œì‹œ
        st.write("### íŠ¹ì„± ìƒ˜í”Œ")
        st.write(features_info["sample_features"])
        
        # ì›í•«ì¸ì½”ë”© ê·¸ë£¹ í‘œì‹œ
        st.write(f"### ì›í•«ì¸ì½”ë”© ê·¸ë£¹ ({features_info['onehot_groups']}ê°œ)")
        
        for group, features in features_info["onehot_features"].items():
            values = [f.split('_', 1)[1] for f in features]
            st.write(f"- **{group}**: {len(features)}ê°œ ê°’")
            st.write(f"  - ê°’ ëª©ë¡: {values}")
            
        # ì›í•«ì¸ì½”ë”© ë˜ì§€ ì•Šì€ íŠ¹ì„±
        if features_info["non_encoded_features"]:
            st.write("### ì›í•«ì¸ì½”ë”© ë˜ì§€ ì•Šì€ íŠ¹ì„±")
            st.write(features_info["non_encoded_features"])
            
        # ì¤‘ìš”ë„ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
        if "top_features" in features_info:
            st.write("### ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±")
            for feature, importance in features_info["top_features"]:
                st.write(f"- {feature}: {importance:.4f}")
                
        # íŠ¹ì„± í˜•ì‹ ì •ë³´
        st.write("### íŠ¹ì„± í˜•ì‹ ì •ë³´")
        st.write(f"- ì–¸ë”ìŠ¤ì½”ì–´(_) í¬í•¨: {features_info['has_underscore_count']}ê°œ")
        st.write(f"- ìˆ«ì í¬í•¨: {features_info['has_digit_count']}ê°œ")
        st.write(f"- ëŒ€ë¬¸ì í¬í•¨: {features_info['has_uppercase_count']}ê°œ")


########## í•¨ìˆ˜ì˜ì—­ì—­ ##########

# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
# MODEL_PATH = Path("models/xgb_best_model.pkl")  # GitHub í”„ë¡œì íŠ¸ ë‚´ ìƒëŒ€ ê²½ë¡œ ì‚¬ìš© ê¶Œì¥

# ===============================
# âœ… ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡ í•¨ìˆ˜
# ===============================
MODEL_PATH = Path(__file__).parent / "xgboost_best_model.pkl"

def load_churn_model(model_path: str = None):
    """
    Load the trained churn prediction model.
    
    Args:
        model_path: Path to the model file. Default is models/xgb_best_model.pkl
        
    Returns:
        Trained model
    """
    if model_path is None:
        model_path = Path(__file__).parent / "xgboost_best_model.pkl"
    else:
        model_path = Path(model_path)
        
    if not model_path.exists():
        raise FileNotFoundError(f"Model file not found: {model_path}")
        
    return joblib.load(model_path)

def predict_churn(model, input_df: pd.DataFrame) -> np.ndarray:
    """
    Predict churn probabilities for input data.
    
    Args:
        model: Trained model
        input_df: Input DataFrame for prediction
        
    Returns:
        np.ndarray: Churn probabilities
    """
    return model.predict_proba(input_df)[:, 1]  # Return churn probabilities

# ===============================
# í† í° ì‹œê°í™” í•¨ìˆ˜
# ===============================

# 1. ì´íƒˆ ë¹„ìœ¨ ì‹œê°í™”
def plot_churn_ratio(df: pd.DataFrame, target_col="Churn"):
    churn_counts = df[target_col].value_counts()
    plt.figure(figsize=(5, 4))
    sns.barplot(x=churn_counts.index, y=churn_counts.values, palette="Set2")
    plt.title("ì´íƒˆ ì—¬ë¶€ ë¹„ìœ¨")
    plt.ylabel("ê³ ê° ìˆ˜")
    return plt.gcf()

# 2. ê³„ì•½ ìœ í˜•ë³„ ì´íƒˆ ë¹„ìœ¨
def plot_churn_by_contract(df: pd.DataFrame, contract_col="Contract", target_col="Churn"):
    plt.figure(figsize=(7, 4))
    sns.countplot(data=df, x=contract_col, hue=target_col, palette="pastel")
    plt.title("ê³„ì•½ ìœ í˜•ë³„ ì´íƒˆ ì—¬ë¶€")
    plt.ylabel("ê³ ê° ìˆ˜")
    plt.xticks(rotation=15)
    return plt.gcf()

# 3. ì˜ˆì¸¡ëœ ì´íƒˆí™•ë¥  ë¶„í¬
def plot_churn_probability_distribution(df: pd.DataFrame, proba_col="ì´íƒˆí™•ë¥ "):
    plt.figure(figsize=(7, 4))
    sns.histplot(df[proba_col], bins=20, kde=True, color="coral")
    plt.title("ì˜ˆì¸¡ëœ ì´íƒˆí™•ë¥  ë¶„í¬")
    plt.xlabel("ì´íƒˆí™•ë¥  (%)")
    return plt.gcf()

# 4. ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë³„ ì´íƒˆì ë¶„í¬ (ex: ë‚˜ì´, ì´ìš©ê°œì›”ìˆ˜)
def plot_churn_by_numeric_feature(df: pd.DataFrame, feature_col="Tenure", target_col="Churn"):
    plt.figure(figsize=(7, 4))
    sns.kdeplot(data=df, x=feature_col, hue=target_col, fill=True, common_norm=False, alpha=0.5)
    plt.title(f"{feature_col}ì— ë”°ë¥¸ ì´íƒˆ ë¶„í¬")
    return plt.gcf()

# 5. ì—¬ëŸ¬ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì— ëŒ€í•œ Boxplot ë¹„êµ
def plot_feature_comparison(df: pd.DataFrame, feature_list, target_col="Churn"):
    n = len(feature_list)
    fig, axes = plt.subplots(nrows=1, ncols=n, figsize=(5*n, 4))
    for i, col in enumerate(feature_list):
        sns.boxplot(x=target_col, y=col, data=df, ax=axes[i], palette="Set2")
        axes[i].set_title(f"{col} vs {target_col}")
    plt.tight_layout()
    return fig

# 6. ë‹¨ì¼ ê³ ê° ì •ë³´ bar chart
def plot_single_customer(df: pd.DataFrame, idx: int):
    row = df.iloc[idx]
    features = row.drop(['ì˜ˆì¸¡ê²°ê³¼', 'ì´íƒˆí™•ë¥ '], errors='ignore')
    plt.figure(figsize=(10, 4))
    features.plot(kind='barh', color='skyblue')
    plt.title(f"ê³ ê° {idx}ë²ˆ íŠ¹ì„± ìš”ì•½")
    plt.tight_layout()
    return plt.gcf()

# 7. SHAP í•´ì„
def explain_shap(model, X_sample: pd.DataFrame):
    explainer = shap.Explainer(model, X_sample)
    shap_values = explainer(X_sample)
    shap.plots.beeswarm(shap_values)

# 8. Feature Importance (ëª¨ë¸ ê¸°ì¤€)
def plot_feature_importance(model, feature_names):
    importance = model.feature_importances_
    fi_df = pd.DataFrame({"Feature": feature_names, "Importance": importance})
    fi_df = fi_df.sort_values(by="Importance", ascending=False)
    plt.figure(figsize=(8, 5))
    sns.barplot(x="Importance", y="Feature", data=fi_df, palette="viridis")
    plt.title("ëª¨ë¸ Feature ì¤‘ìš”ë„")
    plt.tight_layout()
    return plt.gcf()

# 9. ì´íƒˆ ê³ ê° ëŒ€ì‘ ì „ëµ ì¶”ì²œ
def recommend_solution(row):
    strategies = []
    if 'Contract' in row and row['Contract'] == 'Month-to-month':
        strategies.append("2ë…„ ê³„ì•½ ìœ ë„")
    if 'TechSupport' in row and row['TechSupport'] == 'No':
        strategies.append("ê¸°ìˆ  ì§€ì› ì œê³µ")
    if 'OnlineSecurity' in row and row['OnlineSecurity'] == 'No':
        strategies.append("ë³´ì•ˆ ì„œë¹„ìŠ¤ ì¶”ê°€")
    return strategies

# ===============================
# ì¶”ê°€ê°€ ì—…ë°ì´íŠ¸íŠ¸
# ===============================

# 12. ì „ì²´ SHAP í‰ê·  ê¸°ì¤€ ìƒìœ„ feature ë°˜í™˜
def get_top_shap_features(shap_values, X, n=5):
    # ì „ì²´ SHAP ê°’ì—ì„œ í‰ê·  ì˜í–¥ë ¥ì´ í° ìƒìœ„ nê°œ feature ë°˜í™˜
    ...

# 13. ê°œë³„ ê³ ê° SHAP Waterfall ì‹œê°í™”
def plot_waterfall_for_customer(explainer, shap_values, X, idx):
    # ê°œë³„ ê³ ê°ì˜ SHAP ê°’ì„ Waterfall plotìœ¼ë¡œ ì‹œê°í™”
    ...

# 14. ê°œë³„ ê³ ê° ìƒìœ„ ì˜í–¥ feature ë°˜í™˜
def get_customer_top_features(shap_values, X, idx, n=5):
    # íŠ¹ì • ê³ ê°ì˜ ì˜ˆì¸¡ì— ê°€ì¥ í° ì˜í–¥ì„ ì¤€ feature ìƒìœ„ nê°œ ë°˜í™˜
    ...

# ===============================
# ê³ ë‚œì´ë„ í•¨ìˆ˜ ì—…ë°ì´íŠ¸
# ===============================


##########################
# 1. ë°ì´í„°ì…ë ¥
def get_customer_input():
    st.subheader("ê³ ê° ë°ì´í„° ì…ë ¥")

    cols = st.columns(3)

    with cols[0]:
        tenure = st.number_input("ê±°ë˜ê¸°ê°„ (ê°œì›”)", min_value=0, value=12)
        gender = st.selectbox("ì„±ë³„", ["Male", "Female"])
        marital_status = st.selectbox("ê²°í˜¼ ìƒíƒœ", ["Single", "Married"])
        num_orders = st.number_input("ì£¼ë¬¸ íšŸìˆ˜", min_value=0, value=10)
        city_tier = st.number_input("ë„ì‹œ ë“±ê¸‰", min_value=1, max_value=3, value=1)
        registered_devices = st.number_input("ë“±ë¡ëœ ê¸°ê¸° ìˆ˜", min_value=1, value=2)

    with cols[1]:
        preferred_login_device = st.selectbox("ì„ í˜¸ ë¡œê·¸ì¸ ê¸°ê¸°", ["Mobile", "Computer"])
        app_usage = st.number_input("ì•± ì‚¬ìš© ì‹œê°„ (ì‹œê°„)", min_value=0.0, value=3.0)
        address_count = st.number_input("ì£¼ì†Œ ê°œìˆ˜", min_value=0, value=2)
        last_order_days = st.number_input("ë§ˆì§€ë§‰ ì£¼ë¬¸ í›„ ê²½ê³¼ì¼", min_value=0, value=15)
        warehouse_to_home = st.number_input("ì°½ê³ -ì§‘ ê±°ë¦¬ (km)", min_value=0.0, value=20.0)
        satisfaction_score = st.slider("ë§Œì¡±ë„ ì ìˆ˜ (1-5)", min_value=1, max_value=5, value=3)

    with cols[2]:
        preferred_payment = st.selectbox("ì„ í˜¸ ê²°ì œ ë°©ì‹", ["Credit Card", "Debit Card", "Cash on Delivery"])
        preferred_category = st.selectbox("ì„ í˜¸ ì£¼ë¬¸ ì¹´í…Œê³ ë¦¬", ["Electronics", "Clothing", "Groceries"])
        complaints = st.selectbox("ë¶ˆë§Œ ì œê¸° ì—¬ë¶€", ["ì˜ˆ", "ì•„ë‹ˆì˜¤"])
        order_amount_diff = st.number_input("ì‘ë…„ ëŒ€ë¹„ ì£¼ë¬¸ ê¸ˆì•¡ ì¦ê°€ìœ¨ (%)", value=15.0)
        coupon_used = st.number_input("ì¿ í° ì‚¬ìš© íšŸìˆ˜", value=3)
        cashback_amount = st.number_input("ìºì‹œë°± ê¸ˆì•¡ (ì›)", value=150.0)

    input_data = {
        "tenure": tenure,
        "preferred_login_device": preferred_login_device,
        "city_tier": city_tier,
        "warehouse_to_home": warehouse_to_home,
        "preferred_payment_method": preferred_payment,
        "gender": gender,
        "app_usage": app_usage,
        "registered_devices": registered_devices,
        "preferred_order_category": preferred_category,
        "satisfaction_score": satisfaction_score,
        "marital_status": marital_status,
        "address_count": address_count,
        "complaint_status": complaints,
        "order_amount_diff": order_amount_diff,
        "coupon_used": coupon_used,
        "num_orders": num_orders,
        "last_order_days": last_order_days,
        "cashback_amount": cashback_amount
    }

    return input_data

##########################
# 2. ìœ„í—˜í‘œ ì˜ˆì¸¡
def show_churn_risk_dashboard(probability: float):
    """
    ì´íƒˆ í™•ë¥ ì„ ì‹œê°í™”í•˜ê³  ìœ„í—˜ë„ ë° ëŒ€ì‘ ì¡°ì¹˜ë¥¼ ì¶œë ¥
    :param probability: ì˜ˆì¸¡ëœ ì´íƒˆ í™•ë¥  (0~1 ë˜ëŠ” 0~100)
    """

    # 1. í™•ë¥  ì •ê·œí™”
    if probability <= 1.0:
        probability *= 100
    prob = round(probability, 2)

    # 2. ìœ„í—˜ë„ ë“±ê¸‰ íŒì •
    if prob < 30:
        level = "ë‚®ìŒ"
        color = "green"
        recommendation = "ì•ˆì •ì ì¸ ìƒíƒœì…ë‹ˆë‹¤. ì§€ì†ì ì¸ ê´€ë¦¬ë§Œ ìœ ì§€í•˜ë©´ ë©ë‹ˆë‹¤."
    elif prob < 70:
        level = "ì¤‘ê°„"
        color = "orange"
        recommendation = "ì¼ì • ìˆ˜ì¤€ì˜ ë¦¬ìŠ¤í¬ê°€ ìˆìŠµë‹ˆë‹¤. ê³ ê° ë§Œì¡±ë„ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤."
    else:
        level = "ë†’ìŒ"
        color = "red"
        recommendation = "ì¦‰ê°ì ì¸ ê³ ê° ì‘ëŒ€ì™€ íŠ¹ë³„ í˜œíƒ ì œê³µì´ í•„ìš”í•©ë‹ˆë‹¤."

    # 3. ê²Œì´ì§€ ì°¨íŠ¸ ìƒì„±
    fig = go.Figure(go.Indicator(
        mode="gauge+number",
        value=prob,
        number={"suffix": "%"},
        title={"text": "ì´íƒˆ ê°€ëŠ¥ì„± (%)"},
        gauge={
            "axis": {"range": [0, 100]},
            "bar": {"color": "darkblue"},
            "steps": [
                {"range": [0, 30], "color": "green"},
                {"range": [30, 70], "color": "yellow"},
                {"range": [70, 100], "color": "red"},
            ],
        }
    ))

    # 4. Streamlit ì¶œë ¥
    st.subheader("ğŸ“ˆ ì˜ˆì¸¡ ê²°ê³¼")
    st.plotly_chart(fig, use_container_width=True)

    st.subheader("ğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½")
    st.markdown(f"""
    - **ì´íƒˆ í™•ë¥ **: **{prob:.2f}%**  
    - **ìœ„í—˜ë„**: <span style='color:{color}; font-weight:bold'>{level}</span>
    """, unsafe_allow_html=True)

    st.subheader("ğŸ›  ê¶Œì¥ ì¡°ì¹˜")
    st.markdown(f"{recommendation}")


####################################################
# ì´íƒˆ ì˜ˆì¸¡ í•¨ìˆ˜ ëª¨ìŒ
####################################################


# 1. ğŸ“„ ëª¨ë¸ & ë°ì´í„° ë¡œë”©
@st.cache_resource
def load_model_and_data(model_path, data_path):
    model = joblib.load(model_path)
    df = pd.read_pickle(data_path)
    return model, df

# 2. ğŸ“‹ ì»¬ëŸ¼ë³„ ê³ ê° ì •ë³´ ì¶œë ¥
def show_customer_info(customer_row):
    st.subheader("ğŸ“‹ ê³ ê° ì…ë ¥ ë°ì´í„°")
    for col, val in customer_row.items():
        st.write(f"**{col}**: {val}")

# 3. ğŸ¯ ìœ„í—˜ë„ ê²Œì´ì§€ í‘œì‹œ
def show_churn_gauge(prob):
    if prob <= 1: prob *= 100
    risk = round(prob, 2)
    level = "ë†’ìŒ" if risk >= 70 else ("ì¤‘ê°„" if risk >= 30 else "ë‚®ìŒ")
    fig = go.Figure(go.Indicator(
        mode="gauge+number",
        value=risk,
        number={"suffix": "%"},
        title={"text": "ì´íƒˆ ê°€ëŠ¥ì„± (%)"},
        gauge={
            "axis": {"range": [0, 100]},
            "bar": {"color": "darkblue"},
            "steps": [
                {"range": [0, 30], "color": "green"},
                {"range": [30, 70], "color": "yellow"},
                {"range": [70, 100], "color": "red"}
            ]
        }
    ))
    st.subheader("ğŸ“ˆ ì˜ˆì¸¡ ê²°ê³¼")
    st.plotly_chart(fig, use_container_width=True)
    st.markdown(f"**ì˜ˆì¸¡ í™•ë¥ **: {risk:.2f}%  |  **ìœ„í—˜ë„**: :red[{level}]" if level == "ë†’ìŒ" else f"**ì˜ˆì¸¡ í™•ë¥ **: {risk:.2f}%  |  **ìœ„í—˜ë„**: :orange[{level}]" if level == "ì¤‘ê°„" else f"**ì˜ˆì¸¡ í™•ë¥ **: {risk:.2f}%  |  **ìœ„í—˜ë„**: :green[{level}]")

# 4. ğŸ” SHAP ìƒìœ„ 3ê°œ ì˜í–¥ ë³€ìˆ˜ ì‹œê°í™”
def show_top_influencers(model, X_input):
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_input)
    shap_df = pd.DataFrame(shap_values[1], columns=X_input.columns)
    shap_df_mean = shap_df.abs().mean().sort_values(ascending=False).head(3)
    fig = px.bar(x=shap_df_mean.index, y=shap_df_mean.values,
                 labels={'x': 'Feature', 'y': 'SHAP í‰ê·  ì˜í–¥ë„'}, title='ğŸ“Œ ì£¼ìš” ì˜í–¥ ìš”ì¸ Top 3')
    st.plotly_chart(fig, use_container_width=True)





##########################
import joblib
from pathlib import Path

def load_xgboost_model2():
    """
    /models/xgboost_best_model.pkl íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Returns:
        model: í•™ìŠµëœ XGBoost ëª¨ë¸
    Raises:
        FileNotFoundError: ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°
        Exception: ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ
    """
    model_path = Path(__file__).resolve().parent / "xgboost_best_model.pkl"

    if not model_path.exists():
        raise FileNotFoundError(f"[âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ] {model_path}")

    try:
        model = joblib.load(model_path)
        return model
    except Exception as e:
        raise RuntimeError(f"[âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨] {e}")


####################################

# ì´íƒˆíƒ­ ê°’ ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” í¬í•¨ ì½”ë“œ
class ChurnPredictor2:
    """ê³ ê° ì´íƒˆ ì˜ˆì¸¡ì„ ìœ„í•œ ê°œì„ ëœ ëª¨ë¸ í´ë˜ìŠ¤"""

    def __init__(self, model_path=None, external_model=None):
        """ëª¨ë¸ ë¡œë“œ ë˜ëŠ” ì™¸ë¶€ ì£¼ì…"""
        self.model = external_model
        self.model_path = model_path or (Path(__file__).parent / "xgboost_best_model.pkl")
        self.feature_importance_cache = None

        if self.model is None:
            try:
                self.load_model()
            except Exception as e:
                logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                st.error("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")

    def load_model(self):
        """ë¡œì»¬ ëª¨ë¸ íŒŒì¼ ë¡œë“œ"""
        if not self.model_path.exists():
            logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {self.model_path}")
            st.error(f"âŒ ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n{self.model_path}")
            return False
        try:
            self.model = joblib.load(self.model_path)
            logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {self.model_path}")
            return True
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {str(e)}")
            st.error(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜\n{str(e)}")
            return False

    def predict(self, input_df: pd.DataFrame):
        if self.model is None:
            self.load_model()
        if self.model is None:
            st.warning("â— [DEBUG] ëª¨ë¸ì´ ì¡´ì¬í•˜ì§€ ì•Šì•„ ê¸°ë³¸ê°’ ë°˜í™˜ë¨")
            return self._default_prediction()

        try:
            processed_df = self._preprocess_data(input_df)

            # âœ… ë””ë²„ê¹…ìš© ì¶œë ¥
            st.write("âœ… [DEBUG] ì „ì²˜ë¦¬ í›„ ë°ì´í„°:", processed_df)
            st.write("âœ… [DEBUG] ì „ì²˜ë¦¬ ì»¬ëŸ¼ ìˆ˜:", processed_df.shape[1])

            y_pred = self.model.predict(processed_df)
            y_proba = self.model.predict_proba(processed_df)[:, 1]

            st.write("âœ… [DEBUG] ì˜ˆì¸¡ í™•ë¥ :", y_proba)

            if len(y_proba) == 0:
                st.warning("â— [DEBUG] ì˜ˆì¸¡ í™•ë¥ ì´ ë¹„ì–´ ìˆìŒ, ê¸°ë³¸ê°’ ë°˜í™˜")
                return self._default_prediction()

            self.feature_importance_cache = None
            self._compute_feature_importance(processed_df)

            return y_pred, y_proba
        except Exception as e:
            logger.error(f"âŒ ì˜ˆì¸¡ ì˜¤ë¥˜: {str(e)}")
            st.error(f"âŒ [DEBUG] ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._default_prediction()


    def _default_prediction(self):
        """ëª¨ë¸ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë°˜í™˜"""
        return np.array([0]), np.array([0.5])

    def _preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì…ë ¥ ì „ì²˜ë¦¬"""
        df = df.copy()
        for col in ['CustomerID', 'customer_id', 'cust_id', 'id']:
            if col in df.columns:
                df.drop(columns=[col], inplace=True)

        if 'Complain' in df.columns and isinstance(df['Complain'].iloc[0], str):
            df['Complain'] = df['Complain'].apply(lambda x: 1 if x == 'ì˜ˆ' else 0)

        return df

    def _compute_feature_importance(self, input_df: pd.DataFrame):
        """SHAP ê¸°ë°˜ íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°"""
        try:
            if self.model is None:
                return self._default_feature_importance()

            explainer = shap.TreeExplainer(self.model)
            shap_values = explainer.shap_values(input_df)

            if isinstance(shap_values, list):
                shap_values = shap_values[1]  # í´ë˜ìŠ¤ 1 ê¸°ì¤€

            importance = np.abs(shap_values).mean(axis=0)
            self.feature_importance_cache = importance
            return importance
        except Exception as e:
            logger.warning(f"âš ï¸ SHAP ê³„ì‚° ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {str(e)}")
            if hasattr(self.model, 'feature_importances_'):
                self.feature_importance_cache = self.model.feature_importances_
                return self.feature_importance_cache
            return self._default_feature_importance()

    def _default_feature_importance(self):
        """ì¤‘ìš”ë„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜"""
        default_importance = np.array([0.25, 0.22, 0.18, 0.15, 0.12, 0.08])
        self.feature_importance_cache = default_importance
        return default_importance

    def get_feature_importance(self):
        """ìºì‹œëœ ì¤‘ìš”ë„ ë°˜í™˜"""
        if self.feature_importance_cache is None:
            return self._default_feature_importance()

        if isinstance(self.feature_importance_cache, np.ndarray):
            return {
                f'feature_{i+1}': float(val)
                for i, val in enumerate(self.feature_importance_cache)
            }

        return self.feature_importance_cache


########## í•¨ìˆ˜ì—…ë°ì´íŠ¸ì‘ì—… ##########



